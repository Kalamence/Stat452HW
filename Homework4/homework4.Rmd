---
title: "Homework 4"
author: "Matthew Reyers, Dani Chu, Ryan Sheehan"
date: '2017-11-15'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, ISLR, gbm, class, e1071)
data(Caravan)
data(Auto)
```



## Question 1 (Chapter 8, #11, 9 marks)

This question uses the Caravan data set.

(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)

Create a training set consisting of the ﬁrst 1,000 observations, and a test set consisting of the remaining observations. 

```{r}
# Set seed for reproducibility
set.seed(1)

# Train set is first 1000
train.set = Caravan[1:1000, ]

# Test set is the rest
test.set = Caravan[1001:nrow(Caravan), ]
```

(b) (3 marks)

Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?

```{r}
model.boost <- gbm(I(Purchase == "Yes") ~ ., data = train.set, 
                   n.trees = 1000, 
                   distribution = "bernoulli", 
                   shrinkage = 0.01)

# The top 10 most important predictors
summary(model.boost) %>% head(10)
```

(c) (5 marks)

Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r}
# Predict on test data
preds.boost <- predict(model.boost, newdata = test.set, n.trees=1000, type = "response") 
# Purchase if over 20%
preds.boost <- (preds.boost > 0.2) 
# Confusion Matrix
conf.mat.boost <- table(preds.boost, test.set$Purchase)
conf.mat.boost

# Fraction of people predicted to purchase who made one
conf.mat.boost[2,2]/sum(conf.mat.boost[2,])


# Logistic Regression
model.log <- glm(I(Purchase == "Yes") ~ ., data = train.set, family = binomial())
preds.log <- predict(model.log, newdata = test.set, type = "response")
preds.log <- (preds.log > 0.2)
conf.mat.log <- table(preds.log, test.set$Purchase)
conf.mat.log

# Fraction of people predicted to purchase who made one
conf.mat.log[2,2]/sum(conf.mat.log[2,])

# KNN
split = 800
errors <- rep(NA, 20)
x <- train.set[1:split, ] %>% select(-Purchase)
x.test <- train.set[(split + 1):nrow(train.set), ] %>% select(-Purchase)

# tuning k
for(i in 1:20){
  model.knn <- knn(x, 
                   x.test, 
                   cl = train.set[1:split, ]$Purchase,
                   k = i)
  errors[i] = sum(model.knn != train.set[(split+1):nrow(train.set), ]$Purchase)/
              nrow(train.set[(split+1):nrow(train.set), ]) 
}


x <- train.set %>% select(-Purchase)
x.test <- test.set %>% select(-Purchase)
preds.knn <- knn(x, 
                 x.test,
                 cl = train.set$Purchase,
                 k = which.min(errors),
                 prob = TRUE)
# prob reports probability of FALSE
preds.knn <- (attr(preds.knn, "prob") < 0.8)
conf.mat.knn <- table(preds.knn, test.set$Purchase)
conf.mat.knn

# Fraction of people predicted to purchase who made one
conf.mat.knn[2,2]/sum(conf.mat.knn[2,])
```




## Question 2 (Ch9, #7, 11 marks)

In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.

(a) (1 mark)

Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

```{r}
data(Auto)
median <- median(Auto$mpg)
Auto <- Auto %>% mutate(above_med = mpg > median,
                        above_med = above_med %>% as.numeric() %>% as.factor()) %>% select(c(-mpg, -name))
```

(b) (3 mark)

Fit a support vector classiﬁer to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results. 
 
```{r}
set.seed(1)
tune.auto.linear <- tune(svm, above_med ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(10^{-3:3})))

summary(tune.auto.linear)
tune.auto.linear

auto.linear <- svm(above_med ~., data = Auto, type = "C-classification", kernel = "linear", cost = 100)
```
 
 
(c) (5 marks) 

Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with diﬀerent values of gamma and degree and cost. Comment on your results. 

```{r}
set.seed(1)
tune.auto.rad <- tune(svm, above_med ~ ., data = Auto, kernel = "radial", 
                         ranges = list(cost = c(10^{-3:3}),
                                       gamma = c(.5, 1:4)
                                       )
                      )
summary(tune.auto.rad)
tune.auto.rad

auto.rad <- svm(above_med ~., data = Auto, kernel = "radial", cost = 1, gamma = 1)

tune.auto.poly <- tune(svm, above_med ~ ., data = Auto, kernel = "polynomial", 
                         ranges = list(cost = c(10^{-3:3}),
                                       gamma = c(.5, 1:4)
                                       )
                       )
summary(tune.auto.poly)
tune.auto.rad

auto.poly <- svm(above_med ~., data = Auto, kernel = "polynomial", cost = 1, gamma = 1)

```

(d) (2 marks) 

Make some plots to back up your assertions in (b) and (c).
Hint: In the lab, we used the plot() function for svm objects only in cases with p =2. When p>2, you can use the plot() function to create plots displaying pairs of variables at a time. Essentially, instead of typing
"plot(svmfit , dat)" where svmfit contains your ﬁtted model and dat is a data frame containing your data, you can type "plot(svmfit , dat , x1~x4)" in order to plot just the ﬁrst and fourth variables. However, you must replace x1 and x4 with the correct variable names. To ﬁnd out more, type ?plot.svm.

```{r}
plot(auto.linear, data = Auto, acceleration~horsepower)
plot(auto.rad, Auto, acceleration ~ horsepower)

plot(auto.poly, Auto, cylinders ~ origin)
plot(auto.linear)

?plot.svm
colnames(Auto)

plot(auto.linear, data = Auto, weight~year)

for(i in 1:(ncol(Auto)-2)){
  j=i+1
  while(j<=(ncol(Auto)-1)){
    plot(auto.linear, data=Auto, as.formula(paste( colnames(Auto)[i], "~", as.name(colnames(Auto)[j]))))
    j=j+1
  }
}


```

