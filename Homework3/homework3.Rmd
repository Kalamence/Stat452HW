---
title: "Homework 3"
author: "Matthew, Ryan, Dani"
date: '2017-11-01'
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(leaps)
library(dplyr)
library(tidyverse)
library(glmnet)
library(ISLR)
library(pls)
```


## Question 1 (Chapter 6, #8, parts (a)-(e), 10 marks)


(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)

```{r}
set.seed(1234)
n = 100
X = rnorm(n)
error = rnorm(n)
```

(b) (1 mark)

We selected Beta values by the following rule:

$$\beta_i = i+1$$

```{r}
Y = 1 + 2*X + 3*X^2 +4*X^3 + error
```

(c) (3 marks)
For the "best model obtained", you should 
use one that is parsimonious and close to
the consensus best according tht the three
selection criteria.

You don't **have** to create a data frame. 
`regsubsets()` can take a design matrix and
response vector, just like `lm.fit()` and 
`glmnet()`. If you do decide to create a data frame,
the following hint may be of use:

```{r}
pmax <- 10
Xmat <- matrix(NA,nrow=n,ncol=pmax)
for(i in 1:pmax) {
  Xmat[,i] <- X^i
}
colnames(Xmat) <- paste0("X.",1:pmax)
dat <- data.frame(Y,Xmat)
```

# Exhaustive Method

```{r}
models = regsubsets(Y ~ ., data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$adjr2)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))

print(paste("Best model for Adjusted R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", main = "Adjusted R^2 by # of Variables", type="l") 

if(cp.best == bic.best & bic.best == rsq.best){
  print("Best model for all variables has coeffiecients")
  coef(models, cp.best)
} else{
  print("Best model for Cp has coeffiecients")
  coef(models, cp.best)

  print("Best model for BIC  has coeffiecients")
  coef(models, bic.best)

  print("Best model for Adjusted R^2  has coeffiecients")
  coef(models, rsq.best)
}
```


(d) (2 marks) 

# Forward Selection 
```{r}
models = regsubsets(Y ~ ., method = "forward", data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$adjr2)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))
print(paste("Best model for Adjusted R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$adjr2, xlab = "Number of Variables", ylab = "R^2", main = "Adjusted R^2 by # of Variables", type="l") 

if(cp.best == bic.best & bic.best == rsq.best){
  print("Best model for all variables has coeffiecients")
  coef(models, cp.best)
} else{
  print("Best model for Cp has coeffiecients")
  coef(models, cp.best)

  print("Best model for BIC  has coeffiecients")
  coef(models, bic.best)

  print("Best model for Adjusted R^2  has coeffiecients")
  coef(models, rsq.best)
}
```


# Backward Selection 
```{r}
models = regsubsets(Y ~ ., method = "backward", data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$adjr2)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))
print(paste("Best model for Adjusted R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$adjr2, xlab = "Number of Variables", ylab = "Adjusted R^2", main = "Adjusted R^2 by # of Variables", type="l") 

if(cp.best == bic.best & bic.best == rsq.best){
  print("Best model for all variables has coeffiecients")
  coef(models, cp.best)
} else{
  print("Best model for Cp has coeffiecients")
  coef(models, cp.best)

  print("Best model for BIC  has coeffiecients")
  coef(models, bic.best)

  print("Best model for Adjusted R^2  has coeffiecients")
  coef(models, rsq.best)
}
```

# Compare to results in 8c

The results are excatly the same for all 3 selection criteria. The same model was selected by all criteria in all directions.

(e) (3 marks)

```{r}
lambdas <- 10^{seq(from=-2,to=5,length=100)}
cv.lafit <- cv.glmnet(Xmat,Y,alpha=1,lambda=lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best.lam
la.best <- glmnet(Xmat,Y,alpha=1,lambda=la.best.lam)
coef(la.best)
```

# Discuss results obtained from CV and lasso model

The results are almost identical to the model selected above. The coefficients of the predictors in the lasso model have seen a very slight shrinkage the intercept has seen a small increase.

## Question 2 (Ch6, #9, 12 marks)

(a) (0 marks)
To make everyone's results comparable, please
select your test set with the following.

```{r}
data(College)
# Standardize columns
College <- mutate(College,Private = as.numeric(Private=="Yes"))
College <- data.frame(lapply(College,scale))
dim(College) # 777 rows, use 111 as test
set.seed(1)
testset <- sample(1:777,size=111)
College.test <- College[testset,]
College.train <- College[-testset,]
```

(b) (2 marks)

```{r}
linear = lm(Apps ~ . , data = College.train)

preds = predict(linear, College.test)
e1 = mean((College.test$Apps - preds)^2) 
e1
```

(c) (2 marks)

```{r}
lambdas <- 10^{seq(from=-2,to=5,length=100)}
X = College.train %>% select(-Apps) %>% data.matrix()
y = College.train$Apps
cv.lafit <- cv.glmnet(X, y, alpha=0, lambda=lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best <- glmnet(X, y, alpha=0, lambda = la.best.lam)

x.pred = College.test %>% select(-Apps) %>% data.matrix()
preds = predict(la.best, x.pred)
e2 = mean((College.test$Apps - preds)^2)
e2
```

(d) (2 marks)


```{r}
lambdas <- 10^{seq(from=-2,to=5,length=100)}
X = College.train %>% select(-Apps) %>% data.matrix()
y = College.train$Apps
cv.lafit <- cv.glmnet(X, y, alpha = 1, lambda = lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best <- glmnet(X, y, alpha = 1, lambda = la.best.lam)

x.pred = College.test %>% select(-Apps) %>% data.matrix()
preds = predict(la.best, x.pred)
e3 = mean((College.test$Apps - preds)^2)

print(paste("number of coefs larger than 0:", sum(coef(la.best) > 0)))
e3
```

(e) (2 marks)

```{r}
# data already scaled above
pcr.fit = pcr(Apps ∼ ., data = College.train, validation = "CV")

# Choose M by the graph 
validationplot(pcr.fit, val.type = "MSEP")

print("We choose a value of 5 for M is a simple model that has a very good % variance explained")

preds = predict(pcr.fit, College.test, ncomp = 5) 
e4 = mean((preds - College.test$Apps)^2) 
e4
```

(f) (2 marks)

```{r}
pls.fit = plsr(Apps ∼ ., data = College.train, validation = "CV")
# Choose M by the graph and summary
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)
print("We choose a value of 2 for M as it is the simplest model that has a acceptable error rate")

preds = predict(pls.fit, College.test, ncomp = 2) 
e5 = mean((preds - College.test$Apps)^2) 
e5
```

(g) (2 marks)

How accurately can we predict the number of college applications received?

Is there much diﬀerence among the test errors resulting from these ﬁve approaches?

```{r}
e1 #lm
e3 # lasso
e2 # ridge
e5 # pls
e4 # pcr
```

We can predict the number of college applications very closely. The MSE is very small compared to the magnitude of the response variable. Our in sample Adjusted R-squared is 0.92.

The linear model has the smallest MSE for the testing data. It is significantly smaller than that of pcr. There is a spread in MSE across the 5 methods. This indicates that there is a distinct ranking between the 5 methods. 

## Question 3 (Ch7, #6, 8 marks)

(a) (5 marks)

```{r}
attach(Wage)
set.seed(444)

# Make sure train set has full range of age values
Wage <- Wage %>% arrange(age)
Wage.train = Wage[c(1,nrow(Wage)), ]
Wage2 <- Wage[-c(1, nrow(Wage)), ]

# make test set and train set
testset <- sample(1:2998, size = 300)
Wage.test <- Wage2[testset,]
Wage.train <- rbind(Wage2[-testset,], Wage.train)

errors = rep(NA, 8)
for(i in 1:8){
  fit = lm(wage ∼ poly(age, i), data = Wage.test) 
  preds = predict(fit, Wage.train)
  errors[i] = mean((Wage.train$wage - preds)^2) 
}

d = which.min(errors)


fit.1=lm(wage∼age,data=Wage) 
fit.2=lm(wage∼poly(age, 2), data=Wage) 
fit.3=lm(wage∼poly(age, 3), data=Wage) 
fit.4=lm(wage∼poly(age, 4), data=Wage) 
fit.5=lm(wage∼poly(age, 5), data=Wage) 
fit.6=lm(wage∼poly(age, 6), data=Wage) 
fit.7=lm(wage∼poly(age, 7), data=Wage) 
fit.8=lm(wage∼poly(age, 8), data=Wage) 
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8) 

dat = data.frame(Wage$age, fitted(fit.2))


ggplot() +
  geom_point(data = Wage, aes(x = age, y = wage)) +
  geom_line(data = dat, aes(x = Wage.age, y = fitted.fit.2.)) 

```

The degree chose was a polynomial of degree 2. This is confirmed by our anova analysis.

(b) (3 marks)


```{r}
errors = rep(NA, 10)
for(i in 1:10){
  j = i*5
  agebreaks <- seq(17, 80+j, by = j)
  fit = lm(wage ∼ cut(age, breaks = agebreaks), data = Wage.train) 
  preds <-  predict(fit, Wage.test)
  errors[i] <-  mean((Wage.test$wage - preds)^2) 
}


df.errors = data.frame(seq(5,50, by = 5), errors)
colnames(df.errors) = c("group.size","error")

ggplot(df.errors, aes(x = group.size, y =error))+
  geom_point()+
  geom_line()


j = which.min(errors)*5

# age groups should be of size 10
j

agebreaks <- seq(17, 80+j, by = j)
fit = lm(wage ∼ cut(age, breaks = agebreaks), data = Wage) 
uniqueAge <- data.frame(age = sort(unique(Wage$age)))
preds <- data.frame(uniqueAge,predict(fit,newdata = uniqueAge,interval="confidence")) 

ggplot(Wage,aes(x=age,y=wage)) + 
  geom_point(alpha=0.1) + 
  geom_ribbon(aes(x=age,y=fit,ymin=lwr,ymax=upr), data=preds, fill="blue", alpha=.2) + 
  geom_line(aes(y = fit), data = preds, color="blue")

```

