---
title: "Homework 3"
author: "name1 and name2 and name3"
date: '2017-10-25'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Question 1 (Chapter 6, #8, parts (a)-(e), 10 marks)


(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)

```{r}
set.seed(1234)
n = 100
X = rnorm(n)
error = rnorm(n)
```

(b) (1 mark)

```{r}
Y = 1 + 2*X + 3*X^2 +4*X^3 + error
```

(c) (3 marks)
For the "best model obtained", you should 
use one that is parsimonious and close to
the consensus best according tht the three
selection criteria.

You don't **have** to create a data frame. 
`regsubsets()` can take a design matrix and
response vector, just like `lm.fit()` and 
`glmnet()`. If you do decide to create a data frame,
the following hint may be of use:

```{r}
library(leaps)
library(dplyr)
library(tidyverse)
pmax <- 10
Xmat <- matrix(NA,nrow=n,ncol=pmax)
for(i in 1:pmax) {
  Xmat[,i] <- X^i
}
colnames(Xmat) <- paste0("X.",1:pmax)
dat <- data.frame(Y,Xmat)
```

# Exhaustive Method

```{r}
models = regsubsets(Y ~ ., data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$rsq)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))
print(paste("Best model for R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$rsq, xlab = "Number of Variables", ylab = "R^2", main = "R^2 by # of Variables", type="l") 

print("Best model for Cp has coeffiecients")
coef(models, cp.best)

print("Best model for BIC  has coeffiecients")
coef(models, bic.best)

print("Best model for R^2  has coeffiecients")
coef(models, rsq.best)
```


(d) (2 marks) 

# Forward Selection 
```{r}
models = regsubsets(Y ~ ., method = "forward", data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$rsq)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))
print(paste("Best model for R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$rsq, xlab = "Number of Variables", ylab = "R^2", main = "R^2 by # of Variables", type="l") 

print("Best model for Cp has coeffiecients")
coef(models, cp.best)

print("Best model for BIC  has coeffiecients")
coef(models, bic.best)

print("Best model for R^2  has coeffiecients")
coef(models, rsq.best)
```


# Backward Selection 
```{r}
models = regsubsets(Y ~ ., method = "backward", data = dat, nvmax = 10)
models.sum = summary(models)

cp.best = which.min(models.sum$cp)
bic.best = which.min(models.sum$bic)
rsq.best = which.max(models.sum$rsq)

print(paste("Best model for Cp is model with", cp.best, "variables"))
print(paste("Best model for BIC is model with", bic.best, "variables"))
print(paste("Best model for R^2 is model with", rsq.best, "variables"))

plot(models.sum$cp, xlab = "Number of Variables", ylab = "Cp", main = "Cp by # of Variables", type="l") 

plot(models.sum$bic, xlab = "Number of Variables", ylab = "BIC", main = "BIC by # of Variables", type="l") 

plot(models.sum$rsq, xlab = "Number of Variables", ylab = "R^2", main = "R^2 by # of Variables", type="l") 

print("Best model for Cp has coeffiecients")
coef(models, cp.best)

print("Best model for BIC  has coeffiecients")
coef(models, bic.best)

print("Best model for R^2  has coeffiecients")
coef(models, rsq.best)
```

# Compare to results in 8c

The results are excatly the same for all 3 selection criteria.

(e) (3 marks)

```{r}
library(glmnet)
lambdas <- 10^{seq(from=-2,to=5,length=100)}
cv.lafit <- cv.glmnet(Xmat,Y,alpha=1,lambda=lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best.lam
la.best <- glmnet(Xmat,Y,alpha=1,lambda=la.best.lam)
coef(la.best)
```

# Discuss results obtained from CV and lasso model

The results are almost identical to the models selected by BIC and Cp. The coefficients of the predictors in the lasso model have seen a very slight shrinkage the intercept has seen a small increase

## Question 2 (Ch6, #9, 12 marks)

(a) (0 marks)
To make everyone's results comparable, please
select your test set with the following.

```{r}
library(ISLR)
data(College)
# Standardize columns
College <- mutate(College,Private = as.numeric(Private=="Yes"))
College <- data.frame(lapply(College,scale))
dim(College) # 777 rows, use 111 as test
set.seed(1)
testset <- sample(1:777,size=111)
College.test <- College[testset,]
College.train <- College[-testset,]
```

(b) (2 marks)

```{r}
linear = lm(Apps ~ . , data = College.train)

preds = predict(linear, College.test)
mean((College.test$Apps - preds)^2) 
```

(c) (2 marks)

```{r}
lambdas <- 10^{seq(from=-2,to=5,length=100)}
X = College.train %>% select(-Apps) %>% data.matrix()
y = College.train$Apps
cv.lafit <- cv.glmnet(X, y, alpha=0,lambda=lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best <- glmnet(X, y, alpha=0, lambda = la.best.lam)

x.pred = College.test %>% select(-Apps) %>% data.matrix()
preds = predict(la.best, x.pred)
mean((College.test$Apps - preds)^2)
```

(d) (2 marks)


```{r}
lambdas <- 10^{seq(from=-2,to=5,length=100)}
X = College.train %>% select(-Apps) %>% data.matrix()
y = College.train$Apps
cv.lafit <- cv.glmnet(X, y, alpha = 1, lambda = lambdas) 
plot(cv.lafit)
la.best.lam <- cv.lafit$lambda.1se
la.best <- glmnet(X, y, alpha = 1, lambda = la.best.lam)

x.pred = College.test %>% select(-Apps) %>% data.matrix()
preds = predict(la.best, x.pred)
mean((College.test$Apps - preds)^2)

print(paste("number of coefs larger than 0:", sum(coef(la.best) > 0)))
```

(e) (2 marks)

```{r}
library(pls)
pcr.fit = pcr(Apps ∼ ., data = College.train, validation="CV")
# Choose M by the graph 
validationplot(pcr.fit, val.type = "MSEP")

print("We choose a value of 5 for M")

preds = predict(pcr.fit, College.test, ncomp = 5) 
mean((preds - College.test$Apps)^2) 
```

(f) (2 marks)

```{r}
pls.fit = plsr(Apps ∼ ., data = College.train, validation="CV")
# Choose M by the graph and summary
validationplot(pls.fit, val.type = "MSEP")
summary(pls.fit)
print("We choose a value of 2 for M")

preds = predict(pls.fit, College.test, ncomp = 2) 
mean((preds - College.test$Apps)^2) 

```

(g) (2 marks)

How accurately can we predict the number of college applications received?

Is there much diﬀerence among the test errors resulting from these ﬁve approaches?


## Question 3 (Ch7, #6, 8 marks)

(a) (5 marks)

```{r}
attach(Wage)

errors = rep(NA, 8)
for(i in 1:8){
  fit = lm(wage ∼ poly(age, i), data = Wage) 
  preds = predict(fit, Wage)
  errors[i] = mean((Wage$wage - preds)^2) 
}

d = which.min(errors)


fit.1=lm(wage∼age,data=Wage) 
fit.2=lm(wage∼poly(age, 2), data=Wage) 
fit.3=lm(wage∼poly(age, 3), data=Wage) 
fit.4=lm(wage∼poly(age, 4), data=Wage) 
fit.5=lm(wage∼poly(age, 5), data=Wage) 
fit.6=lm(wage∼poly(age, 6), data=Wage) 
fit.7=lm(wage∼poly(age, 7), data=Wage) 
fit.8=lm(wage∼poly(age, 8), data=Wage) 
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8) 

dat = data.frame(Wage$age, fitted(fit.8))


ggplot() +
  geom_point(data = Wage, aes(x = Wage$age, y = Wage$wage)) +
  geom_line(data = dat, aes(x = Wage.age, y = fitted.fit.8.)) 

```

(b) (3 marks)


```{r}

```

(b) Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of the ﬁt obtained.

